{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a04bd2eaf942411dbd70e72365517fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d395656543343db8abcfe9b76817bcd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_22787cc6d90b4b76b971d4a5499da072",
              "IPY_MODEL_7b81683a68c64d0fac9bb86315dc4683"
            ]
          }
        },
        "4d395656543343db8abcfe9b76817bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22787cc6d90b4b76b971d4a5499da072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14890d36d7434fc295490ea950483a31",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79e8517bc28d48fd9f6cd79e192b5c8e"
          }
        },
        "7b81683a68c64d0fac9bb86315dc4683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7be6d6bee35c457caa6c2d3ecfd914e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [56:03&lt;00:00, 50697.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6aabba60aaf840448a319f4071fe7fb7"
          }
        },
        "14890d36d7434fc295490ea950483a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79e8517bc28d48fd9f6cd79e192b5c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7be6d6bee35c457caa6c2d3ecfd914e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6aabba60aaf840448a319f4071fe7fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc4ee38fbea94deeb1935b6faeddd03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bbff97b0ce19446b8a4e63864c017e01",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_643385fae9034e88b6da2b00903c8725",
              "IPY_MODEL_3cfaffe724e845edbd27c84b5a5dff7f"
            ]
          }
        },
        "bbff97b0ce19446b8a4e63864c017e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "643385fae9034e88b6da2b00903c8725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f15327b5be0457fbb23bdf5ad5a1228",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac71a0510799489397fcf8e96e1b8373"
          }
        },
        "3cfaffe724e845edbd27c84b5a5dff7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c9406eace134bb6abb7f31874949650",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:15&lt;00:00, 10801147.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b871952a476c481a934fd9f60f813370"
          }
        },
        "5f15327b5be0457fbb23bdf5ad5a1228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac71a0510799489397fcf8e96e1b8373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c9406eace134bb6abb7f31874949650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b871952a476c481a934fd9f60f813370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5576f9796ec949f3a4a988bc459527ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c34dd8d9550b4b6b9d0bcb8a7c1e2d50",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6b49a741d4640faa6a577cdcfe7475b",
              "IPY_MODEL_93b13c5797974eeba1a44fd160a66729"
            ]
          }
        },
        "c34dd8d9550b4b6b9d0bcb8a7c1e2d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6b49a741d4640faa6a577cdcfe7475b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_df496bb938cc43b69332ab31439c0ac9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c6ee8839e414e75ad076d4436768808"
          }
        },
        "93b13c5797974eeba1a44fd160a66729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c34f2ccb08494f46bb4d3f86c4017291",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [55:18&lt;00:00, 167kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dbc067f36b04292a3aa758cc83bcee4"
          }
        },
        "df496bb938cc43b69332ab31439c0ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c6ee8839e414e75ad076d4436768808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c34f2ccb08494f46bb4d3f86c4017291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dbc067f36b04292a3aa758cc83bcee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbP5eHDNd-Mc"
      },
      "source": [
        "$\\color{blue}{\\text{What is Neural Network }}$\n",
        "A neural network is simply a group of connected neurons, there are some input neurons, some output neurons and a group of what we call hidden neurons in between. When we feed information to the input neurons we get some information from the output neurons. Information starts at the input neurons and travels to the next layers of neurons having whats called a weight and a bias applied to it. These weight and biases start out randomly determined and are tweaked as the network learns and sees more data. After reaching a new layer there is a function applied to each neurons value that is called an activation function.\n",
        "\n",
        "\n",
        "$\\color{blue}{\\text{How does Neural Network work? }}$\n",
        "\n",
        "y=w1*x1+w2*x2+w3*x3+w4(bias)\n",
        "z=act(y)\n",
        "\n",
        "First weights will be passed to hidden neurons, and two steps would happen. \n",
        "1. The summation of weights and features would happen and then bias is added\n",
        "2.  Activation function is applied to the summation of weights\n",
        "\n",
        "$\\color{blue}{\\text{How does activation function work? }}$\n",
        "Lets suppose, if we keep our one hand on a hot object, then the neurons of that hand would get activated, but not of the other hands \n",
        "1. Sigmoid Activation Function: used in Logistic Regression, and Activation function is 1/(1+e^-y). Any value of y given to sigmoid would result either in 0 or in 1. There is not in between value. \n",
        "2. ReLu Activation Function: the product and summation of weights, features, and bias is sent to Relu, and max(y, 0) is found in Relu.\n",
        "\n",
        "$\\color{blue}{\\text{Neural Network Training }}$\n",
        "All the input features will pass to the hidden layer, then the two steps of how neural network works are followed and output layer produces the predicted output. y^=predicted value, y=actual value\n",
        "Then loss function is applied. The difference of y-y^ is found. The loss function value should be reduced to minimal value so that y=y^. We can do this by updating weights and using optimizer. To achieve the minimal loss value, we got to update the weights using backpropagation. Lets suppose we want to update the bias w4, $\\color{blue}{\\text{w4'=w4-learning_rate* derivative of w4}}$ subsequently, w1, w2, w3 would be updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7b2iatQIx5b"
      },
      "source": [
        "from torchvision import datasets as ds\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision as tv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv01OA_rI72e"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVpnPdy8izR"
      },
      "source": [
        "###  $\\color{blue}{\\text{Normalize does the following for each channel:}}$\n",
        "\n",
        "image = (image - mean) / std\n",
        "\n",
        "The parameters mean, std are passed as 0.5, 0.5 in your case. This will normalize the image in the range [-1,1]. For example, the minimum value 0 will be converted to (0-0.5)/0.5=-1, the maximum value of 1 will be converted to (1-0.5)/0.5=1. Both parameters are “Sequences for each channel”. Color images have three channels (red, green, blue), therefore you need three parameters to normalize each channel. The first tuple (0.5, 0.5, 0.5) is the mean for all three channels and the second (0.5, 0.5, 0.5) is the standard deviation for all three channels.\n",
        "\n",
        "if you would like to get your image back in [0,1] range, you could use,\n",
        "\n",
        "image = ((image * std) + mean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "a04bd2eaf942411dbd70e72365517fa1",
            "4d395656543343db8abcfe9b76817bcd",
            "22787cc6d90b4b76b971d4a5499da072",
            "7b81683a68c64d0fac9bb86315dc4683",
            "14890d36d7434fc295490ea950483a31",
            "79e8517bc28d48fd9f6cd79e192b5c8e",
            "7be6d6bee35c457caa6c2d3ecfd914e6",
            "6aabba60aaf840448a319f4071fe7fb7"
          ]
        },
        "id": "DCtUpnxkJUw0",
        "outputId": "ae72c195-7401-4a20-df02-161b92f9e78e"
      },
      "source": [
        "train_set = ds.CIFAR10(root='../input/', train=True, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "testset = tv.datasets.CIFAR10(root='../input/', train=False, download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,  batch_size=4, shuffle=True, num_workers=0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../input/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a04bd2eaf942411dbd70e72365517fa1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../input/cifar-10-python.tar.gz to ../input/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVgHrHiB7KQg"
      },
      "source": [
        "CIFAR10 in torch package has 60,000 images of 10 labels, with the size of 32x32 pixels. By default, torchvision.datasets.CIFAR10 will separate the dataset into 50,000 images for training and 10,000 images for testing.\n",
        "\n",
        "$\\color{blue}{\\text{Dataset loader }}$\n",
        "The dataset is divided in three categories: training, validation and test. The first one will be, obviously, used for trainig; the validation set will be used to measure the model performance during training and the test set will be used to evaluate our model performance once the training has finished.\n",
        "\n",
        "$\\color{blue}{\\text{Utils}}$\n",
        "Some utility function to visualize the dataset and the model's prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU76FYQDKCsW"
      },
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XVVJaJNPmO"
      },
      "source": [
        "## $\\color{blue}{\\text{VGG16 Architecture}}$\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1dimQsscYoFA63KN1FDsWe7bwIrvxpRhR)\n",
        "\n",
        "1. VGG: The VGG16 is a CNN model that instead of having a large number of hyper-parameter they focused on having convolution layers of 3x3 filter with a stride 1 and always used same padding and maxpool layer of 2x2 filter of stride 2. It follows this arrangement of convolution and max pool layers consistently throughout the whole architecture. In the end it has 2 FC(fully connected layers) followed by a softmax for output. The 16 in VGG16 refers to it has 16 layers that have weights. This network is a pretty large network and it has about 138 million (approx) parameters.\n",
        "2. I will be using Sequential method as I am creating a sequential model. Sequential model means that all the layers of the model will be arranged in sequence. \n",
        "\n",
        "\n",
        "  a. torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "\n",
        "stride controls the stride for the cross-correlation, a single number or a tuple.\n",
        "\n",
        "padding controls the amount of implicit padding on both sides for padding number of points for each dimension.\n",
        "\n",
        "in_channels (int) – Number of channels in the input image\n",
        "\n",
        "out_channels (int) – Number of channels produced by the convolution\n",
        "\n",
        "kernel_size (int or tuple) – Size of the convolving kernel\n",
        "\n",
        "stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
        "\n",
        "padding (int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0\n",
        "\n",
        "$\\color{blue}{\\text{relu(Rectified Linear Unit) activation to each layers so that all the negative values are not passed to the next layer. }}$\n",
        "\n",
        "the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 3×3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv.  layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2×2 pixel window, with stride 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5DT8QabKJh0"
      },
      "source": [
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(3, 32, 3, padding=1),  # Conv1\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),  # Conv2\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool1\n",
        "            nn.Conv2d(32, 64, 3, padding=1),  # Conv3\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),  # Conv4\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool2\n",
        "            nn.Conv2d(64, 128, 3, padding=1),  # Conv5\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),  # Conv6\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),  # Conv7\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool3\n",
        "            nn.Conv2d(128, 256, 3, padding=1),  # Conv8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv9\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv10\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool4\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv11\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv12\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv13\n",
        "            nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2, 2)  # Pool5 \n",
        "        )\n",
        "#Sequential object from keras. A Sequential model simply defines a sequence of layers starting with the input layer and ending with the \n",
        "#output layer. Our model will have 3 layers, and input layer of 784 neurons (representing all of the 28x28 pixels in a picture) a hidden \n",
        "#layer of an arbitrary 128 neurons and an output layer of 10 neurons representing the probability of the picture being each of the 10 classes.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * 2 * 256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNuy4ayBLRwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a18787-d34e-4531-af3b-b254d5df46e3"
      },
      "source": [
        "net = VGGNet(16)\n",
        "net.cuda()\n",
        "lr = 1e-3\n",
        "momentum = 0.9\n",
        "num_epoch = 50\n",
        "\n",
        "critierion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "print('Training with learning rate = %f, momentum = %f ' % (lr, momentum))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with learning rate = 0.001000, momentum = 0.900000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFe112kv9A6G"
      },
      "source": [
        "n_total_step in my case is 1,250 steps, it is calculated by <total records>/<batch size>, so my case is$\\color{blue}{\\text{ 50,000/40 = 1,250. it means that in training stage, each epoch my code will execute a loop of 1,250 steps.}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG5AmdlO9iY1"
      },
      "source": [
        "$\\color{red}{\\text{CrossEntropyLoss}}$ $\\color{blue}{\\text{function in torch to calculate the loss value. This function received the predicted y value of n-features and the labels and does the}}$  $\\color{blue}{\\text{ softmax calculation, in my case, I have 10-feature predicted outputs for each image.}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AekB1LP7LeQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641a032b-757f-4456-de81-80d890f616ff"
      },
      "source": [
        "loss_p = np.array([])\n",
        "for t in range(num_epoch):\n",
        "    running_loss = 0\n",
        "    running_loss_sum_per_epoch = 0\n",
        "    total_images = 0\n",
        "    correct_images = 0\n",
        "    if t == 25:\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr/10, momentum=momentum)\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        images, labels = data\n",
        "        images = Variable(images.cuda())\n",
        "        labels = Variable(labels.cuda())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        _, predicts = torch.max(outputs.data, 1)\n",
        "        loss = critierion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_images += labels.size(0)\n",
        "        correct_images += (predicts == labels).sum().item()\n",
        "        loss_data = loss.data.item()\n",
        "        running_loss += loss_data\n",
        "        \n",
        "        running_loss_sum_per_epoch = running_loss + running_loss_sum_per_epoch\n",
        "        if i % 2000 == 1999:\n",
        "            print('Epoch, batch [%d, %5d] loss: %.6f, Training accuracy: %.5f' %\n",
        "                  (t + 1, i + 1, running_loss / 2000, 100 * correct_images / total_images))\n",
        "            running_loss = 0\n",
        "            total_images = 0\n",
        "            correct_images = 0\n",
        "\n",
        "    loss_p = np.append(loss_p, running_loss_sum_per_epoch)\n",
        "\n",
        "print('Finished training.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch, batch [1,  2000] loss: 2.536399, Training accuracy: 10.48750\n",
            "Epoch, batch [1,  4000] loss: 2.333488, Training accuracy: 10.18750\n",
            "Epoch, batch [1,  6000] loss: 2.244716, Training accuracy: 15.25000\n",
            "Epoch, batch [1,  8000] loss: 2.070428, Training accuracy: 18.42500\n",
            "Epoch, batch [1, 10000] loss: 1.977798, Training accuracy: 19.43750\n",
            "Epoch, batch [1, 12000] loss: 1.936953, Training accuracy: 20.28750\n",
            "Epoch, batch [2,  2000] loss: 1.875491, Training accuracy: 24.26250\n",
            "Epoch, batch [2,  4000] loss: 1.787872, Training accuracy: 28.47500\n",
            "Epoch, batch [2,  6000] loss: 1.746399, Training accuracy: 30.95000\n",
            "Epoch, batch [2,  8000] loss: 1.702240, Training accuracy: 32.63750\n",
            "Epoch, batch [2, 10000] loss: 1.668457, Training accuracy: 32.86250\n",
            "Epoch, batch [2, 12000] loss: 1.610163, Training accuracy: 36.57500\n",
            "Epoch, batch [3,  2000] loss: 1.554412, Training accuracy: 39.67500\n",
            "Epoch, batch [3,  4000] loss: 1.510543, Training accuracy: 42.42500\n",
            "Epoch, batch [3,  6000] loss: 1.461720, Training accuracy: 44.93750\n",
            "Epoch, batch [3,  8000] loss: 1.396178, Training accuracy: 48.45000\n",
            "Epoch, batch [3, 10000] loss: 1.345849, Training accuracy: 50.71250\n",
            "Epoch, batch [3, 12000] loss: 1.294607, Training accuracy: 53.68750\n",
            "Epoch, batch [4,  2000] loss: 1.216469, Training accuracy: 56.08750\n",
            "Epoch, batch [4,  4000] loss: 1.190911, Training accuracy: 57.33750\n",
            "Epoch, batch [4,  6000] loss: 1.150722, Training accuracy: 59.87500\n",
            "Epoch, batch [4,  8000] loss: 1.101289, Training accuracy: 61.47500\n",
            "Epoch, batch [4, 10000] loss: 1.099875, Training accuracy: 60.86250\n",
            "Epoch, batch [4, 12000] loss: 1.064511, Training accuracy: 62.42500\n",
            "Epoch, batch [5,  2000] loss: 0.997255, Training accuracy: 65.31250\n",
            "Epoch, batch [5,  4000] loss: 0.965664, Training accuracy: 66.31250\n",
            "Epoch, batch [5,  6000] loss: 0.954992, Training accuracy: 67.23750\n",
            "Epoch, batch [5,  8000] loss: 0.952139, Training accuracy: 66.78750\n",
            "Epoch, batch [5, 10000] loss: 0.937411, Training accuracy: 67.60000\n",
            "Epoch, batch [5, 12000] loss: 0.923070, Training accuracy: 68.91250\n",
            "Epoch, batch [6,  2000] loss: 0.824883, Training accuracy: 71.80000\n",
            "Epoch, batch [6,  4000] loss: 0.828500, Training accuracy: 71.92500\n",
            "Epoch, batch [6,  6000] loss: 0.830467, Training accuracy: 72.01250\n",
            "Epoch, batch [6,  8000] loss: 0.802288, Training accuracy: 72.61250\n",
            "Epoch, batch [6, 10000] loss: 0.804355, Training accuracy: 73.21250\n",
            "Epoch, batch [6, 12000] loss: 0.810582, Training accuracy: 72.88750\n",
            "Epoch, batch [7,  2000] loss: 0.688275, Training accuracy: 76.36250\n",
            "Epoch, batch [7,  4000] loss: 0.713673, Training accuracy: 75.83750\n",
            "Epoch, batch [7,  6000] loss: 0.731088, Training accuracy: 75.41250\n",
            "Epoch, batch [7,  8000] loss: 0.712121, Training accuracy: 76.23750\n",
            "Epoch, batch [7, 10000] loss: 0.721006, Training accuracy: 76.20000\n",
            "Epoch, batch [7, 12000] loss: 0.723517, Training accuracy: 75.83750\n",
            "Epoch, batch [8,  2000] loss: 0.612255, Training accuracy: 80.08750\n",
            "Epoch, batch [8,  4000] loss: 0.639250, Training accuracy: 79.20000\n",
            "Epoch, batch [8,  6000] loss: 0.657996, Training accuracy: 77.92500\n",
            "Epoch, batch [8,  8000] loss: 0.640441, Training accuracy: 79.13750\n",
            "Epoch, batch [8, 10000] loss: 0.652492, Training accuracy: 78.03750\n",
            "Epoch, batch [8, 12000] loss: 0.630690, Training accuracy: 79.13750\n",
            "Epoch, batch [9,  2000] loss: 0.532421, Training accuracy: 82.45000\n",
            "Epoch, batch [9,  4000] loss: 0.561756, Training accuracy: 81.60000\n",
            "Epoch, batch [9,  6000] loss: 0.577939, Training accuracy: 80.68750\n",
            "Epoch, batch [9,  8000] loss: 0.566839, Training accuracy: 81.21250\n",
            "Epoch, batch [9, 10000] loss: 0.576299, Training accuracy: 80.56250\n",
            "Epoch, batch [9, 12000] loss: 0.617990, Training accuracy: 79.32500\n",
            "Epoch, batch [10,  2000] loss: 0.493898, Training accuracy: 83.38750\n",
            "Epoch, batch [10,  4000] loss: 0.487841, Training accuracy: 84.26250\n",
            "Epoch, batch [10,  6000] loss: 0.497412, Training accuracy: 83.43750\n",
            "Epoch, batch [10,  8000] loss: 0.526236, Training accuracy: 82.51250\n",
            "Epoch, batch [10, 10000] loss: 0.532937, Training accuracy: 82.28750\n",
            "Epoch, batch [10, 12000] loss: 0.516678, Training accuracy: 83.35000\n",
            "Epoch, batch [11,  2000] loss: 0.434843, Training accuracy: 85.81250\n",
            "Epoch, batch [11,  4000] loss: 0.468226, Training accuracy: 84.67500\n",
            "Epoch, batch [11,  6000] loss: 0.451883, Training accuracy: 85.41250\n",
            "Epoch, batch [11,  8000] loss: 0.468799, Training accuracy: 84.92500\n",
            "Epoch, batch [11, 10000] loss: 0.484932, Training accuracy: 83.81250\n",
            "Epoch, batch [11, 12000] loss: 0.483693, Training accuracy: 83.82500\n",
            "Epoch, batch [12,  2000] loss: 0.361728, Training accuracy: 88.08750\n",
            "Epoch, batch [12,  4000] loss: 0.407372, Training accuracy: 86.48750\n",
            "Epoch, batch [12,  6000] loss: 0.411990, Training accuracy: 86.46250\n",
            "Epoch, batch [12,  8000] loss: 0.425484, Training accuracy: 86.03750\n",
            "Epoch, batch [12, 10000] loss: 0.423075, Training accuracy: 86.15000\n",
            "Epoch, batch [12, 12000] loss: 0.456345, Training accuracy: 84.97500\n",
            "Epoch, batch [13,  2000] loss: 0.350101, Training accuracy: 88.55000\n",
            "Epoch, batch [13,  4000] loss: 0.363038, Training accuracy: 88.23750\n",
            "Epoch, batch [13,  6000] loss: 0.374745, Training accuracy: 87.81250\n",
            "Epoch, batch [13,  8000] loss: 0.380120, Training accuracy: 87.08750\n",
            "Epoch, batch [13, 10000] loss: 0.392631, Training accuracy: 87.07500\n",
            "Epoch, batch [13, 12000] loss: 0.389852, Training accuracy: 87.51250\n",
            "Epoch, batch [14,  2000] loss: 0.316135, Training accuracy: 90.01250\n",
            "Epoch, batch [14,  4000] loss: 0.340469, Training accuracy: 89.06250\n",
            "Epoch, batch [14,  6000] loss: 0.339240, Training accuracy: 88.68750\n",
            "Epoch, batch [14,  8000] loss: 0.356940, Training accuracy: 88.42500\n",
            "Epoch, batch [14, 10000] loss: 0.357626, Training accuracy: 88.32500\n",
            "Epoch, batch [14, 12000] loss: 0.408073, Training accuracy: 86.68750\n",
            "Epoch, batch [15,  2000] loss: 0.291453, Training accuracy: 90.33750\n",
            "Epoch, batch [15,  4000] loss: 0.285351, Training accuracy: 90.80000\n",
            "Epoch, batch [15,  6000] loss: 0.337831, Training accuracy: 88.82500\n",
            "Epoch, batch [15,  8000] loss: 0.324848, Training accuracy: 89.45000\n",
            "Epoch, batch [15, 10000] loss: 0.334954, Training accuracy: 88.88750\n",
            "Epoch, batch [15, 12000] loss: 0.351613, Training accuracy: 88.56250\n",
            "Epoch, batch [16,  2000] loss: 0.280653, Training accuracy: 90.98750\n",
            "Epoch, batch [16,  4000] loss: 0.281877, Training accuracy: 91.10000\n",
            "Epoch, batch [16,  6000] loss: 0.305215, Training accuracy: 90.06250\n",
            "Epoch, batch [16,  8000] loss: 0.313018, Training accuracy: 90.06250\n",
            "Epoch, batch [16, 10000] loss: 0.344294, Training accuracy: 88.61250\n",
            "Epoch, batch [16, 12000] loss: 0.323608, Training accuracy: 89.08750\n",
            "Epoch, batch [17,  2000] loss: 0.226033, Training accuracy: 92.57500\n",
            "Epoch, batch [17,  4000] loss: 0.256768, Training accuracy: 91.75000\n",
            "Epoch, batch [17,  6000] loss: 0.278478, Training accuracy: 90.98750\n",
            "Epoch, batch [17,  8000] loss: 0.293615, Training accuracy: 90.72500\n",
            "Epoch, batch [17, 10000] loss: 0.314618, Training accuracy: 90.35000\n",
            "Epoch, batch [17, 12000] loss: 0.316608, Training accuracy: 90.23750\n",
            "Epoch, batch [18,  2000] loss: 0.253131, Training accuracy: 91.92500\n",
            "Epoch, batch [18,  4000] loss: 0.251553, Training accuracy: 92.08750\n",
            "Epoch, batch [18,  6000] loss: 0.249695, Training accuracy: 91.90000\n",
            "Epoch, batch [18,  8000] loss: 0.278827, Training accuracy: 91.25000\n",
            "Epoch, batch [18, 10000] loss: 0.260817, Training accuracy: 91.58750\n",
            "Epoch, batch [18, 12000] loss: 0.294716, Training accuracy: 90.50000\n",
            "Epoch, batch [19,  2000] loss: 0.200486, Training accuracy: 93.68750\n",
            "Epoch, batch [19,  4000] loss: 0.255116, Training accuracy: 92.11250\n",
            "Epoch, batch [19,  6000] loss: 0.254195, Training accuracy: 92.23750\n",
            "Epoch, batch [19,  8000] loss: 0.269262, Training accuracy: 91.51250\n",
            "Epoch, batch [19, 10000] loss: 0.267084, Training accuracy: 91.67500\n",
            "Epoch, batch [19, 12000] loss: 0.296549, Training accuracy: 90.91250\n",
            "Epoch, batch [20,  2000] loss: 0.208309, Training accuracy: 93.37500\n",
            "Epoch, batch [20,  4000] loss: 0.227258, Training accuracy: 92.86250\n",
            "Epoch, batch [20,  6000] loss: 0.243719, Training accuracy: 92.48750\n",
            "Epoch, batch [20,  8000] loss: 0.286009, Training accuracy: 90.70000\n",
            "Epoch, batch [20, 10000] loss: 0.260603, Training accuracy: 91.92500\n",
            "Epoch, batch [20, 12000] loss: 0.243450, Training accuracy: 92.62500\n",
            "Epoch, batch [21,  2000] loss: 0.203764, Training accuracy: 94.08750\n",
            "Epoch, batch [21,  4000] loss: 0.219157, Training accuracy: 93.47500\n",
            "Epoch, batch [21,  6000] loss: 0.223014, Training accuracy: 93.11250\n",
            "Epoch, batch [21,  8000] loss: 0.237968, Training accuracy: 92.23750\n",
            "Epoch, batch [21, 10000] loss: 0.221064, Training accuracy: 93.35000\n",
            "Epoch, batch [21, 12000] loss: 0.275913, Training accuracy: 91.63750\n",
            "Epoch, batch [22,  2000] loss: 0.167756, Training accuracy: 94.73750\n",
            "Epoch, batch [22,  4000] loss: 0.251748, Training accuracy: 92.53750\n",
            "Epoch, batch [22,  6000] loss: 0.214578, Training accuracy: 93.56250\n",
            "Epoch, batch [22,  8000] loss: 0.256276, Training accuracy: 92.03750\n",
            "Epoch, batch [22, 10000] loss: 0.252151, Training accuracy: 92.35000\n",
            "Epoch, batch [22, 12000] loss: 0.274877, Training accuracy: 91.51250\n",
            "Epoch, batch [23,  2000] loss: 0.180406, Training accuracy: 94.35000\n",
            "Epoch, batch [23,  4000] loss: 0.209267, Training accuracy: 93.81250\n",
            "Epoch, batch [23,  6000] loss: 0.209292, Training accuracy: 93.72500\n",
            "Epoch, batch [23,  8000] loss: 0.259178, Training accuracy: 92.00000\n",
            "Epoch, batch [23, 10000] loss: 0.264046, Training accuracy: 91.95000\n",
            "Epoch, batch [23, 12000] loss: 0.237818, Training accuracy: 92.60000\n",
            "Epoch, batch [24,  2000] loss: 0.199634, Training accuracy: 94.03750\n",
            "Epoch, batch [24,  4000] loss: 0.223978, Training accuracy: 93.13750\n",
            "Epoch, batch [24,  6000] loss: 0.244544, Training accuracy: 92.60000\n",
            "Epoch, batch [24,  8000] loss: 0.253108, Training accuracy: 92.32500\n",
            "Epoch, batch [24, 10000] loss: 0.265504, Training accuracy: 92.26250\n",
            "Epoch, batch [24, 12000] loss: 0.257577, Training accuracy: 91.88750\n",
            "Epoch, batch [25,  2000] loss: 0.178165, Training accuracy: 94.68750\n",
            "Epoch, batch [25,  4000] loss: 0.192831, Training accuracy: 94.33750\n",
            "Epoch, batch [25,  6000] loss: 0.203592, Training accuracy: 93.73750\n",
            "Epoch, batch [25,  8000] loss: 0.224381, Training accuracy: 93.33750\n",
            "Epoch, batch [25, 10000] loss: 0.250991, Training accuracy: 93.13750\n",
            "Epoch, batch [25, 12000] loss: 0.263131, Training accuracy: 92.42500\n",
            "Epoch, batch [26,  2000] loss: 0.121261, Training accuracy: 96.42500\n",
            "Epoch, batch [26,  4000] loss: 0.096977, Training accuracy: 97.13750\n",
            "Epoch, batch [26,  6000] loss: 0.074237, Training accuracy: 97.86250\n",
            "Epoch, batch [26,  8000] loss: 0.069153, Training accuracy: 98.00000\n",
            "Epoch, batch [26, 10000] loss: 0.080638, Training accuracy: 97.66250\n",
            "Epoch, batch [26, 12000] loss: 0.058227, Training accuracy: 98.16250\n",
            "Epoch, batch [27,  2000] loss: 0.029369, Training accuracy: 99.30000\n",
            "Epoch, batch [27,  4000] loss: 0.028706, Training accuracy: 99.22500\n",
            "Epoch, batch [27,  6000] loss: 0.030097, Training accuracy: 99.16250\n",
            "Epoch, batch [27,  8000] loss: 0.027217, Training accuracy: 99.22500\n",
            "Epoch, batch [27, 10000] loss: 0.029908, Training accuracy: 99.18750\n",
            "Epoch, batch [27, 12000] loss: 0.023572, Training accuracy: 99.42500\n",
            "Epoch, batch [28,  2000] loss: 0.012873, Training accuracy: 99.75000\n",
            "Epoch, batch [28,  4000] loss: 0.017032, Training accuracy: 99.57500\n",
            "Epoch, batch [28,  6000] loss: 0.011077, Training accuracy: 99.76250\n",
            "Epoch, batch [28,  8000] loss: 0.011815, Training accuracy: 99.67500\n",
            "Epoch, batch [28, 10000] loss: 0.014873, Training accuracy: 99.71250\n",
            "Epoch, batch [28, 12000] loss: 0.013792, Training accuracy: 99.66250\n",
            "Epoch, batch [29,  2000] loss: 0.007342, Training accuracy: 99.82500\n",
            "Epoch, batch [29,  4000] loss: 0.007918, Training accuracy: 99.86250\n",
            "Epoch, batch [29,  6000] loss: 0.007904, Training accuracy: 99.86250\n",
            "Epoch, batch [29,  8000] loss: 0.010445, Training accuracy: 99.75000\n",
            "Epoch, batch [29, 10000] loss: 0.005173, Training accuracy: 99.91250\n",
            "Epoch, batch [29, 12000] loss: 0.008747, Training accuracy: 99.82500\n",
            "Epoch, batch [30,  2000] loss: 0.007964, Training accuracy: 99.83750\n",
            "Epoch, batch [30,  4000] loss: 0.004658, Training accuracy: 99.95000\n",
            "Epoch, batch [30,  6000] loss: 0.004014, Training accuracy: 99.93750\n",
            "Epoch, batch [30,  8000] loss: 0.004497, Training accuracy: 99.90000\n",
            "Epoch, batch [30, 10000] loss: 0.006196, Training accuracy: 99.80000\n",
            "Epoch, batch [30, 12000] loss: 0.003273, Training accuracy: 99.95000\n",
            "Epoch, batch [31,  2000] loss: 0.002940, Training accuracy: 99.95000\n",
            "Epoch, batch [31,  4000] loss: 0.002681, Training accuracy: 99.95000\n",
            "Epoch, batch [31,  6000] loss: 0.003676, Training accuracy: 99.93750\n",
            "Epoch, batch [31,  8000] loss: 0.004342, Training accuracy: 99.90000\n",
            "Epoch, batch [31, 10000] loss: 0.004132, Training accuracy: 99.91250\n",
            "Epoch, batch [31, 12000] loss: 0.002678, Training accuracy: 99.93750\n",
            "Epoch, batch [32,  2000] loss: 0.002440, Training accuracy: 99.96250\n",
            "Epoch, batch [32,  4000] loss: 0.002807, Training accuracy: 99.97500\n",
            "Epoch, batch [32,  6000] loss: 0.001991, Training accuracy: 99.95000\n",
            "Epoch, batch [32,  8000] loss: 0.002805, Training accuracy: 99.93750\n",
            "Epoch, batch [32, 10000] loss: 0.002820, Training accuracy: 99.92500\n",
            "Epoch, batch [32, 12000] loss: 0.003367, Training accuracy: 99.93750\n",
            "Epoch, batch [33,  2000] loss: 0.002091, Training accuracy: 99.97500\n",
            "Epoch, batch [33,  4000] loss: 0.001499, Training accuracy: 99.97500\n",
            "Epoch, batch [33,  6000] loss: 0.002821, Training accuracy: 99.93750\n",
            "Epoch, batch [33,  8000] loss: 0.002369, Training accuracy: 99.95000\n",
            "Epoch, batch [33, 10000] loss: 0.001295, Training accuracy: 99.96250\n",
            "Epoch, batch [33, 12000] loss: 0.003167, Training accuracy: 99.95000\n",
            "Epoch, batch [34,  2000] loss: 0.002085, Training accuracy: 99.95000\n",
            "Epoch, batch [34,  4000] loss: 0.001208, Training accuracy: 99.98750\n",
            "Epoch, batch [34,  6000] loss: 0.002294, Training accuracy: 99.93750\n",
            "Epoch, batch [34,  8000] loss: 0.001829, Training accuracy: 99.97500\n",
            "Epoch, batch [34, 10000] loss: 0.000838, Training accuracy: 100.00000\n",
            "Epoch, batch [34, 12000] loss: 0.001045, Training accuracy: 99.97500\n",
            "Epoch, batch [35,  2000] loss: 0.001493, Training accuracy: 99.97500\n",
            "Epoch, batch [35,  4000] loss: 0.002749, Training accuracy: 99.93750\n",
            "Epoch, batch [35,  6000] loss: 0.002023, Training accuracy: 99.95000\n",
            "Epoch, batch [35,  8000] loss: 0.001439, Training accuracy: 99.97500\n",
            "Epoch, batch [35, 10000] loss: 0.000831, Training accuracy: 99.98750\n",
            "Epoch, batch [35, 12000] loss: 0.000465, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  2000] loss: 0.000978, Training accuracy: 99.97500\n",
            "Epoch, batch [36,  4000] loss: 0.000535, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  6000] loss: 0.001573, Training accuracy: 99.96250\n",
            "Epoch, batch [36,  8000] loss: 0.001241, Training accuracy: 99.97500\n",
            "Epoch, batch [36, 10000] loss: 0.001973, Training accuracy: 99.97500\n",
            "Epoch, batch [36, 12000] loss: 0.001143, Training accuracy: 99.98750\n",
            "Epoch, batch [37,  2000] loss: 0.000487, Training accuracy: 100.00000\n",
            "Epoch, batch [37,  4000] loss: 0.000755, Training accuracy: 99.97500\n",
            "Epoch, batch [37,  6000] loss: 0.000514, Training accuracy: 100.00000\n",
            "Epoch, batch [37,  8000] loss: 0.001842, Training accuracy: 99.95000\n",
            "Epoch, batch [37, 10000] loss: 0.001763, Training accuracy: 99.97500\n",
            "Epoch, batch [37, 12000] loss: 0.000461, Training accuracy: 100.00000\n",
            "Epoch, batch [38,  2000] loss: 0.000712, Training accuracy: 99.98750\n",
            "Epoch, batch [38,  4000] loss: 0.000759, Training accuracy: 99.97500\n",
            "Epoch, batch [38,  6000] loss: 0.001101, Training accuracy: 99.98750\n",
            "Epoch, batch [38,  8000] loss: 0.001037, Training accuracy: 99.96250\n",
            "Epoch, batch [38, 10000] loss: 0.001023, Training accuracy: 99.98750\n",
            "Epoch, batch [38, 12000] loss: 0.000902, Training accuracy: 99.97500\n",
            "Epoch, batch [39,  2000] loss: 0.001105, Training accuracy: 99.96250\n",
            "Epoch, batch [39,  4000] loss: 0.000406, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  6000] loss: 0.000377, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  8000] loss: 0.000977, Training accuracy: 99.97500\n",
            "Epoch, batch [39, 10000] loss: 0.001202, Training accuracy: 99.98750\n",
            "Epoch, batch [39, 12000] loss: 0.000482, Training accuracy: 99.98750\n",
            "Epoch, batch [40,  2000] loss: 0.000910, Training accuracy: 99.98750\n",
            "Epoch, batch [40,  4000] loss: 0.000451, Training accuracy: 99.98750\n",
            "Epoch, batch [40,  6000] loss: 0.000285, Training accuracy: 100.00000\n",
            "Epoch, batch [40,  8000] loss: 0.000243, Training accuracy: 100.00000\n",
            "Epoch, batch [40, 10000] loss: 0.000620, Training accuracy: 100.00000\n",
            "Epoch, batch [40, 12000] loss: 0.001076, Training accuracy: 99.97500\n",
            "Epoch, batch [41,  2000] loss: 0.000454, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  4000] loss: 0.000457, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  6000] loss: 0.000332, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  8000] loss: 0.000747, Training accuracy: 99.98750\n",
            "Epoch, batch [41, 10000] loss: 0.000427, Training accuracy: 100.00000\n",
            "Epoch, batch [41, 12000] loss: 0.000904, Training accuracy: 99.97500\n",
            "Epoch, batch [42,  2000] loss: 0.000730, Training accuracy: 99.98750\n",
            "Epoch, batch [42,  4000] loss: 0.000832, Training accuracy: 99.98750\n",
            "Epoch, batch [42,  6000] loss: 0.000382, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  8000] loss: 0.000862, Training accuracy: 99.97500\n",
            "Epoch, batch [42, 10000] loss: 0.000193, Training accuracy: 100.00000\n",
            "Epoch, batch [42, 12000] loss: 0.000270, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  2000] loss: 0.000174, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  4000] loss: 0.000595, Training accuracy: 99.98750\n",
            "Epoch, batch [43,  6000] loss: 0.000579, Training accuracy: 99.98750\n",
            "Epoch, batch [43,  8000] loss: 0.000641, Training accuracy: 99.98750\n",
            "Epoch, batch [43, 10000] loss: 0.000177, Training accuracy: 100.00000\n",
            "Epoch, batch [43, 12000] loss: 0.000379, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  2000] loss: 0.000461, Training accuracy: 99.98750\n",
            "Epoch, batch [44,  4000] loss: 0.000195, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  6000] loss: 0.000358, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  8000] loss: 0.000444, Training accuracy: 100.00000\n",
            "Epoch, batch [44, 10000] loss: 0.000374, Training accuracy: 100.00000\n",
            "Epoch, batch [44, 12000] loss: 0.000430, Training accuracy: 99.98750\n",
            "Epoch, batch [45,  2000] loss: 0.000452, Training accuracy: 99.98750\n",
            "Epoch, batch [45,  4000] loss: 0.000253, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  6000] loss: 0.000231, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  8000] loss: 0.000355, Training accuracy: 99.98750\n",
            "Epoch, batch [45, 10000] loss: 0.000412, Training accuracy: 99.98750\n",
            "Epoch, batch [45, 12000] loss: 0.000437, Training accuracy: 99.98750\n",
            "Epoch, batch [46,  2000] loss: 0.000410, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  4000] loss: 0.000485, Training accuracy: 99.98750\n",
            "Epoch, batch [46,  6000] loss: 0.000233, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  8000] loss: 0.000172, Training accuracy: 100.00000\n",
            "Epoch, batch [46, 10000] loss: 0.000445, Training accuracy: 99.98750\n",
            "Epoch, batch [46, 12000] loss: 0.000168, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  2000] loss: 0.000226, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  4000] loss: 0.000261, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  6000] loss: 0.000178, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  8000] loss: 0.000327, Training accuracy: 99.98750\n",
            "Epoch, batch [47, 10000] loss: 0.000434, Training accuracy: 99.98750\n",
            "Epoch, batch [47, 12000] loss: 0.000258, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  2000] loss: 0.000272, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  4000] loss: 0.000308, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  6000] loss: 0.000381, Training accuracy: 99.98750\n",
            "Epoch, batch [48,  8000] loss: 0.000218, Training accuracy: 100.00000\n",
            "Epoch, batch [48, 10000] loss: 0.000360, Training accuracy: 99.98750\n",
            "Epoch, batch [48, 12000] loss: 0.000188, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  2000] loss: 0.000531, Training accuracy: 99.97500\n",
            "Epoch, batch [49,  4000] loss: 0.000141, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  6000] loss: 0.000197, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  8000] loss: 0.000122, Training accuracy: 100.00000\n",
            "Epoch, batch [49, 10000] loss: 0.000243, Training accuracy: 100.00000\n",
            "Epoch, batch [49, 12000] loss: 0.000170, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  2000] loss: 0.000220, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  4000] loss: 0.000111, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  6000] loss: 0.000434, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  8000] loss: 0.000238, Training accuracy: 100.00000\n",
            "Epoch, batch [50, 10000] loss: 0.000314, Training accuracy: 99.98750\n",
            "Epoch, batch [50, 12000] loss: 0.000169, Training accuracy: 100.00000\n",
            "Finished training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_b4VS_YJ_G2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462c9f46-126b-4356-ef4f-89d2fbe919be"
      },
      "source": [
        "with torch.no_grad():\n",
        "    number_corrects = 0\n",
        "    number_samples = 0\n",
        "    for i, (test_images_set , test_labels_set) in enumerate(testloader):\n",
        "        test_images_set = test_images_set.cuda()\n",
        "        test_labels_set = test_labels_set.cuda()\n",
        "    \n",
        "        y_predicted = net(test_images_set)\n",
        "        labels_predicted = y_predicted.argmax(axis = 1)\n",
        "        number_corrects += (labels_predicted==test_labels_set).sum().item()\n",
        "        number_samples += test_labels_set.size(0)\n",
        "    print(f'Overall accuracy {(number_corrects / number_samples)*100}%')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall accuracy 81.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmatJRUwOhJg"
      },
      "source": [
        " $\\color{blue}{\\text{Return evenly spaced numbers over a specified interval.}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "Z_VpD_X3OnRf",
        "outputId": "15a03c20-72d6-4530-cb50-c45d2e26c581"
      },
      "source": [
        "e = np.linspace(0, num_epoch, num_epoch)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a64228d765a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JduADGniOBCs"
      },
      "source": [
        "plt.plot(e, loss_p, color='red', linestyle='--', labels='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_39ROXyQaQyW"
      },
      "source": [
        " ## $\\color{blue}{\\text{Theory Behind the CNN}}$\n",
        " In a CNN, the input is a tensor with a shape: (number of inputs) x (input height) x (input width) x (input channels). After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) x (feature map height) x (feature map width) x (feature map channels). A convolutional layer within a CNN generally has the following attributes:\n",
        "\n",
        "Convolutional filters/kernels defined by a width and height (hyper-parameters).\n",
        "The number of input channels and output channels (hyper-parameters). One layer's input channels must equal the number of output channels (also called depth) of its input.\n",
        "Additional hyperparameters of the convolution operation, such as: padding, stride, and dilation.\n",
        "Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[14] Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs such as high resolution images. It would require a very high number of neurons, even in a shallow architecture, due to the large input size of images, where each pixel is a relevant input feature. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper.[15] For example, regardless of image size, using a 5 x 5 tiling region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in traditional neural networks.[16][17] Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling.\n",
        "\n",
        "Pooling layers\n",
        "Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 x 2 are commonly used. Global pooling acts on all the neurons of the feature map.[18][19] There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map,[20][21] while average pooling takes the average value.\n",
        "\n",
        "Fully connected layers\n",
        "Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n",
        "\n",
        "Receptive field\n",
        "In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes into account the value of a pixel, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\n",
        "\n",
        "Weights\n",
        "Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\n",
        "\n",
        "The vector of weights and the bias are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.\n",
        "\n",
        "ReLU layer\n",
        "ReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function {\\textstyle f(x)=\\max(0,x)}{\\textstyle f(x)=\\max(0,x)}.[56] It effectively removes negative values from an activation map by setting them to zero.[69] It introduces nonlinearities to the decision function and in the overall network without affecting the receptive fields of the convolution layers.\n",
        "\n",
        "Other functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent {\\displaystyle f(x)=\\tanh(x)}{\\displaystyle f(x)=\\tanh(x)}, {\\displaystyle f(x)=|\\tanh(x)|}{\\displaystyle f(x)=|\\tanh(x)|}, and the sigmoid function {\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[70]\n",
        "\n",
        "Fully connected layer\n",
        "After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n",
        "\n",
        "CNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.\n",
        "\n",
        "Number of filters\n",
        "Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n",
        "\n",
        "The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n",
        "\n",
        "Filter size\n",
        "Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set.\n",
        "\n",
        "The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.\n",
        "\n",
        "Pooling type and size\n",
        "In modern CNNs, max pooling is typically used, and often of size 2×2, with a stride of 2. This implies that the input is drastically downsampled, further improving the computational efficiency.\n",
        "\n",
        "Very large input volumes may warrant 4×4 pooling in the lower layers.[71] However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tog0sO-DZ6ZE"
      },
      "source": [
        "##  $\\color{blue}{\\text{Using Pretrained Model From Torchvision}}$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAR9Pv5wgA2a",
        "outputId": "52ccbeeb-d950-425a-8461-b20ef29e1db0"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "num_epochs = 5\n",
        "batch_size = 40\n",
        "learning_rate = 0.001\n",
        "classes = ('plane', 'car' , 'bird',\n",
        "    'cat', 'deer', 'dog',\n",
        "    'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "dc4ee38fbea94deeb1935b6faeddd03e",
            "bbff97b0ce19446b8a4e63864c017e01",
            "643385fae9034e88b6da2b00903c8725",
            "3cfaffe724e845edbd27c84b5a5dff7f",
            "5f15327b5be0457fbb23bdf5ad5a1228",
            "ac71a0510799489397fcf8e96e1b8373",
            "2c9406eace134bb6abb7f31874949650",
            "b871952a476c481a934fd9f60f813370"
          ]
        },
        "id": "sI8DE5pbgSXs",
        "outputId": "f3227238-252e-46e7-d245-c4e871b0fdac"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize( \n",
        "       (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) \n",
        "    )\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = True,\n",
        "    download =True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = False,\n",
        "    download =True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc4ee38fbea94deeb1935b6faeddd03e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDOhZye3gSaF",
        "outputId": "68b3f151-7617-43f8-b4de-4fedf97b3e91"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset\n",
        "    , batch_size = batch_size\n",
        "    , shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset\n",
        "    , batch_size = batch_size\n",
        "    , shuffle = True)\n",
        "n_total_step = len(train_loader)\n",
        "print(n_total_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "5576f9796ec949f3a4a988bc459527ef",
            "c34dd8d9550b4b6b9d0bcb8a7c1e2d50",
            "d6b49a741d4640faa6a577cdcfe7475b",
            "93b13c5797974eeba1a44fd160a66729",
            "df496bb938cc43b69332ab31439c0ac9",
            "4c6ee8839e414e75ad076d4436768808",
            "c34f2ccb08494f46bb4d3f86c4017291",
            "5dbc067f36b04292a3aa758cc83bcee4"
          ]
        },
        "id": "6B16-ZdagSdF",
        "outputId": "26f0c1eb-387f-4365-9fb4-2a05474b4ea5"
      },
      "source": [
        "model = models.vgg16(pretrained = True)\n",
        "input_lastLayer = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(input_lastLayer,10)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9,weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5576f9796ec949f3a4a988bc459527ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZDO24apgSfb",
        "outputId": "4d90fe27-c7f7-4b1f-f69f-55dab5bda8ec"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for i, (imgs , labels) in enumerate(train_loader):\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    labels_hat = model(imgs)\n",
        "    n_corrects = (labels_hat.argmax(axis=1)==labels).sum().item()\n",
        "    loss_value = criterion(labels_hat, labels)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if(i+1)%250==0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {loss_value:.5f}, acc = {100*(n_corrects/labels.size(0)):.2f}%')\n",
        "  print()\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/5, step: 250/1250: loss = 0.01601, acc = 100.00%\n",
            "epoch 1/5, step: 500/1250: loss = 0.03563, acc = 97.50%\n",
            "epoch 1/5, step: 750/1250: loss = 0.14697, acc = 95.00%\n",
            "epoch 1/5, step: 1000/1250: loss = 0.06332, acc = 97.50%\n",
            "epoch 1/5, step: 1250/1250: loss = 0.01718, acc = 100.00%\n",
            "\n",
            "epoch 2/5, step: 250/1250: loss = 0.01432, acc = 100.00%\n",
            "epoch 2/5, step: 500/1250: loss = 0.00585, acc = 100.00%\n",
            "epoch 2/5, step: 750/1250: loss = 0.00607, acc = 100.00%\n",
            "epoch 2/5, step: 1000/1250: loss = 0.00969, acc = 100.00%\n",
            "epoch 2/5, step: 1250/1250: loss = 0.08723, acc = 97.50%\n",
            "\n",
            "epoch 3/5, step: 250/1250: loss = 0.03872, acc = 100.00%\n",
            "epoch 3/5, step: 500/1250: loss = 0.00851, acc = 100.00%\n",
            "epoch 3/5, step: 750/1250: loss = 0.01612, acc = 100.00%\n",
            "epoch 3/5, step: 1000/1250: loss = 0.01277, acc = 100.00%\n",
            "epoch 3/5, step: 1250/1250: loss = 0.01875, acc = 100.00%\n",
            "\n",
            "epoch 4/5, step: 250/1250: loss = 0.05796, acc = 95.00%\n",
            "epoch 4/5, step: 500/1250: loss = 0.00648, acc = 100.00%\n",
            "epoch 4/5, step: 750/1250: loss = 0.08531, acc = 97.50%\n",
            "epoch 4/5, step: 1000/1250: loss = 0.00319, acc = 100.00%\n",
            "epoch 4/5, step: 1250/1250: loss = 0.07378, acc = 95.00%\n",
            "\n",
            "epoch 5/5, step: 250/1250: loss = 0.00876, acc = 100.00%\n",
            "epoch 5/5, step: 500/1250: loss = 0.00485, acc = 100.00%\n",
            "epoch 5/5, step: 750/1250: loss = 0.01254, acc = 100.00%\n",
            "epoch 5/5, step: 1000/1250: loss = 0.17313, acc = 95.00%\n",
            "epoch 5/5, step: 1250/1250: loss = 0.00151, acc = 100.00%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ1DtPWWgSiP",
        "outputId": "f9a307a2-e716-49c8-f3f0-0fedfc24da66"
      },
      "source": [
        "with torch.no_grad():\n",
        "    number_corrects = 0\n",
        "    number_samples = 0\n",
        "    for i, (test_images_set , test_labels_set) in enumerate(test_loader):\n",
        "        test_images_set = test_images_set.to(device)\n",
        "        test_labels_set = test_labels_set.to(device)\n",
        "    \n",
        "        y_predicted = model(test_images_set)\n",
        "        labels_predicted = y_predicted.argmax(axis = 1)\n",
        "        number_corrects += (labels_predicted==test_labels_set).sum().item()\n",
        "        number_samples += test_labels_set.size(0)\n",
        "    print(f'Overall accuracy {(number_corrects / number_samples)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall accuracy 93.47999999999999%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7Z48RugSko"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u68ewf1ZgSqH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybJ2aKLyJ6uf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3xqMSTZvls3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}